import: [lookup_embedder]

surge:
  transformer_impl: pytorch
  use_preln: false
  use_rat: false
  dim: -1
  nlayer: 2
  nhead: 2

  # default values match the same number in pytorch implementation
  ff_dim: 2048 # transformer FFN dimension
  hidden_dropout: 0.1 # transformer hidden dropout
  attn_dropout: 0.1 # transformer attention dropout
  ctx_dropout: 0.5 # neighbor discrete dropout
  output_dropout: 0.0 # entity embedding dropout in output
  rel_dropout: 0.0
  self_dropout: 0.0
  mlm_mask: 0.6
  mlm_replace: 0.3
  add_mlm_loss: true
  activation: relu
  max_context_size: 0
  similarity: DotProduct
  initializer_range: 0.02 # transformer Linear and Embedding weight normal distribution std
  
  # Dynamic temporal subgraph parameters
  adaptive_window: false # Enable adaptive time windowing
  base_time_window: 10 # Base time window size
  max_time_window: 40 # Maximum adaptive window size
  temporal_decay: 0.15 # Temporal decay factor for weighting
  max_temporal_hops: 2 # Maximum temporal hops for expansion
  
  class_name: SURGE
  entity_embedder:
    type: lookup_embedder
    +++: +++
  relation_embedder:
    type: lookup_embedder
    +++: +++
  time_embedder:
    type: lookup_embedder
    +++: +++
