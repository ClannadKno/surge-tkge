job.type: train
dataset.name: wikidata
model: surge

random_seed:
  python: 333
  torch: 339
  numpy: 333

eval:
  batch_size: 512
  metrics_per:
    relation_type: true
  trace_level: example

train:
  loss: kl
  auto_correct: true
  batch_size: 512
  max_epochs: 1000
  lr_scheduler: "" #set empty here to use schedule below
  loss_arg: .nan # disable label smoothing
  optimizer: Adamax
  optimizer_args:
    lr: 0.01
    warmup: 0.1
    schedule: warmup_linear_xdl
    eps: 1e-6
    weight_decay: 0.01
    max_grad_norm: 1.0
  type: 1vsAll

lookup_embedder:
  dim: 320
  dropout: 0.
  initialize: normal_
  initialize_args:
    normal_:
      std: 0.02
surge:
  # === Dynamic Temporal Subgraph Parameters ===
  # Enable adaptive time windowing
  adaptive_window: true
  # Base time window size (will be adapted based on context)
  base_time_window: 15
  # Maximum adaptive window size
  max_time_window: 60
  # Temporal decay factor for weighting distant neighbors
  temporal_decay: 0.15
  # Maximum number of temporal hops for subgraph expansion
  max_temporal_hops: 2

  # === Original SURGE Parameters ===
  transformer_impl: huggingface
  activation: gelu
  hidden_dropout: 0.1
  attn_dropout: 0.1
  ctx_dropout: 0.3
  self_dropout: 0.5
  max_context_size: 50
  mlm_mask: 1.0
  mlm_replace: 0.3 # no effect since mlm_mask=1
  add_mlm_loss: true
  output_dropout: 0.6
  nlayer: 6
  nhead: 8
  ff_dim: 1024
  
valid:
  early_stopping:
    min_threshold:
      epochs: 50
      metric_value: 0.05
    patience: 10